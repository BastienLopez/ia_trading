# Guide des Tests - Crypto Trading AI

Guide dÃ©taillÃ© pour l'exÃ©cution et la maintenance des tests du projet.

## ğŸ“‹ Vue d'ensemble

Le projet utilise pytest comme framework de test principal, avec:
- Tests unitaires pour chaque composant
- Tests d'intÃ©gration pour les interactions entre composants
- Mesure de la couverture de code avec pytest-cov

## ğŸ”§ Installation des outils de test

```bash
# Installation des dÃ©pendances de test
pip install pytest pytest-cov
```

## ğŸš€ ExÃ©cution des tests

### Tests complets

Pour exÃ©cuter tous les tests:

```bash
python -m pytest
```

### Tests par module

Pour tester un module spÃ©cifique:

```bash
# Tests du module d'IA
python -m pytest tests/ai_trading/

# Tests du bot Discord
python -m pytest tests/discord_bot/

# Tests du module de configuration
python -m pytest tests/test_config.py
```

### Tests individuels

Pour exÃ©cuter un test spÃ©cifique:

```bash
# Test d'une classe spÃ©cifique
python -m pytest tests/ai_trading/test_data_processor.py::TestDataProcessor

# Test d'une mÃ©thode spÃ©cifique
python -m pytest tests/ai_trading/test_data_processor.py::TestDataProcessor::test_add_indicators
```

### Couverture de code

Pour mesurer la couverture de code:

```bash
python -m pytest --cov=ai_trading --cov=discord_bot --cov-report=term-missing
```

## ğŸ“ Utilisation des logs et prints pour le dÃ©bogage

Pour faciliter le dÃ©bogage des tests, vous pouvez ajouter des logs ou des prints qui vous aideront Ã  comprendre l'exÃ©cution:

### Utilisation des prints simples

```python
def test_add_indicators(self):
    """Test de l'ajout d'indicateurs techniques"""
    # Affichage des informations de dÃ©bogage
    print(f"\nTest dÃ©marrÃ©: {self._testMethodName}")
    print(f"Forme du DataFrame initial: {self.test_df.shape}")
    
    # ExÃ©cution du test
    result = self.data_processor.add_indicators(self.test_df.copy())
    
    # Affichage des rÃ©sultats intermÃ©diaires
    print(f"Colonnes du DataFrame rÃ©sultant: {result.columns.tolist()}")
    print(f"Forme du DataFrame rÃ©sultant: {result.shape}")
    
    # VÃ©rification des indicateurs...
```

Pour activer l'affichage des prints pendant les tests:

```bash
python -m pytest -v tests/ai_trading/test_data_processor.py::TestDataProcessor::test_add_indicators -s
```

Le paramÃ¨tre `-s` est essentiel car il dÃ©sactive la capture de la sortie standard, permettant aux messages de s'afficher.

### Utilisation du logging

Le logging est plus flexible que les prints simples:

```python
import logging

# Configuration du logger au dÃ©but du fichier de test
logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.DEBUG, 
                   format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')

class TestDataProcessor(unittest.TestCase):
    def test_add_indicators(self):
        logger.debug("Test dÃ©marrÃ©: %s", self._testMethodName)
        logger.info("Forme du DataFrame initial: %s", self.test_df.shape)
        
        # ExÃ©cution du test
        result = self.data_processor.add_indicators(self.test_df.copy())
        
        logger.debug("Colonnes rÃ©sultantes: %s", result.columns.tolist())
        # ...
```

Pour activer les logs pendant les tests:

```bash
# DÃ©finir le niveau de logging pour les tests
LOGLEVEL=DEBUG python -m pytest tests/ai_trading/test_data_processor.py -v
```

### DÃ©bogage avancÃ© avec des assertions dÃ©taillÃ©es

Pour des tests plus lisibles et informatifs:

```python
def test_crypto_trading_env_step_buy(self):
    """Test de l'action 'acheter' dans l'environnement"""
    self.env.reset()
    initial_balance = self.env.balance
    print(f"\n[DÃ‰BOGAGE] Solde initial: {initial_balance}")
    
    # Action 1 = Acheter
    observation, reward, done, truncated, info = self.env.step(1)
    print(f"[DÃ‰BOGAGE] AprÃ¨s achat - Solde: {self.env.balance}, Crypto: {self.env.crypto_held}")
    print(f"[DÃ‰BOGAGE] RÃ©compense: {reward}, Position: {self.env.current_position}")
    print(f"[DÃ‰BOGAGE] Valeur du portefeuille: {self.env.portfolio_value}")
    
    # VÃ©rifications avec messages explicites
    self.assertEqual(self.env.balance, 0, 
                    f"Le solde devrait Ãªtre 0 aprÃ¨s achat mais est {self.env.balance}")
    self.assertGreater(self.env.crypto_held, 0, 
                      f"Cryptos dÃ©tenus devraient Ãªtre > 0 mais sont {self.env.crypto_held}")
```

### Utilisation de fichiers de log

Pour garder une trace permanente des exÃ©cutions de test:

```python
import logging
from datetime import datetime

# Configuration du logger avec fichier
log_file = f"logs/test_run_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(log_file),
        logging.StreamHandler()  # Affiche aussi dans la console
    ]
)

def setUp(self):
    """Configuration avant chaque test"""
    logging.info("====== DÃ©but du test: %s ======", self._testMethodName)
    # ...

def tearDown(self):
    """Nettoyage aprÃ¨s chaque test"""
    logging.info("====== Fin du test: %s ======\n", self._testMethodName)
    # ...
```

## ğŸ§© Structure des tests

```
tests/
â”œâ”€â”€ conftest.py              # Configuration et fixtures communes
â”œâ”€â”€ test_config.py           # Tests de configuration
â”œâ”€â”€ __init__.py              # Initialisation du module de tests
â”œâ”€â”€ ai_trading/              # Tests du module d'IA
â”‚   â”œâ”€â”€ test_api.py          # Tests de l'API
â”‚   â”œâ”€â”€ test_rl_agent.py     # Tests de l'agent RL
â”‚   â”œâ”€â”€ test_data_processor.py # Tests du traitement des donnÃ©es
â”‚   â””â”€â”€ __init__.py
â””â”€â”€ discord_bot/             # Tests du bot Discord
    â”œâ”€â”€ test_bot.py          # Tests du bot
    â””â”€â”€ __init__.py
```

## ğŸ“Š Tests de l'API

Les tests de l'API utilisent TestClient de FastAPI pour simuler des requÃªtes HTTP:

```python
def test_predict_endpoint_success(self, mock_get_agent, mock_get_data_processor):
    """Test de l'endpoint /predict avec succÃ¨s"""
    # Configuration...
    print(f"\n[DÃ‰BOGAGE] Test de l'endpoint /predict - Envoi d'une requÃªte")
    response = self.client.post("/predict", json={"symbol": "BTC/USDT"})
    print(f"[DÃ‰BOGAGE] Code de statut: {response.status_code}")
    print(f"[DÃ‰BOGAGE] RÃ©ponse: {response.json()}")
    
    self.assertEqual(response.status_code, 200)
    # Assertions...
```

## ğŸ¤– Tests de l'Agent RL

Les tests de l'agent RL vÃ©rifient son comportement avec diffÃ©rentes actions de trading:

```python
def test_crypto_trading_env_step_buy(self):
    """Test de l'action 'acheter' dans l'environnement"""
    # Configuration...
    logging.info("Test de l'action Acheter - Ã‰tape initiale")
    observation, reward, done, truncated, info = self.env.step(1)  # Action 1 = Acheter
    logging.info("AprÃ¨s action Acheter - Solde: %.2f, Crypto: %.8f, Position: %d", 
                self.env.balance, self.env.crypto_held, self.env.current_position)
    # Assertions...
```

## ğŸ“‰ Tests du Traitement des DonnÃ©es

Les tests du processeur de donnÃ©es vÃ©rifient les calculs d'indicateurs techniques:

```python
def test_add_indicators(self):
    """Test de l'ajout d'indicateurs techniques"""
    result = self.data_processor.add_indicators(self.test_df.copy())
    # VÃ©rification des indicateurs...
```

## ğŸ“¨ Tests du Bot Discord

Les tests du bot Discord simulent des commandes et vÃ©rifient les rÃ©ponses:

```python
async def test_price_command(self, mock_bot, mock_ctx):
    """Test de la commande !prix"""
    await mock_bot.price.callback(mock_bot, mock_ctx, symbol="BTC")
    mock_ctx.send.assert_called_once()
    # VÃ©rifications...
```

## âš ï¸ RÃ©solution des problÃ¨mes

### DÃ©pendances manquantes

Si certains tests Ã©chouent en raison de dÃ©pendances manquantes:

```bash
# Installer les dÃ©pendances
pip install ccxt gymnasium matplotlib
```

### Erreurs d'importation

Si vous rencontrez des erreurs d'importation dans les tests:

1. VÃ©rifiez que les modules sont correctement installÃ©s
2. VÃ©rifiez la variable PYTHONPATH
3. Assurez-vous que les `__init__.py` sont prÃ©sents dans chaque rÃ©pertoire

### Mocks pour les API externes

Les tests utilisent des mocks pour les services externes (API de change, Discord).
Assurez-vous que les mocks sont correctement configurÃ©s.

### DÃ©buggez un test spÃ©cifique avec pdb

Pour dÃ©bugger interactivement un test:

```bash
python -m pytest tests/ai_trading/test_data_processor.py::TestDataProcessor::test_add_indicators --pdb
```

Vous pouvez aussi ajouter des points d'arrÃªt dans votre code de test:

```python
def test_add_indicators(self):
    """Test de l'ajout d'indicateurs techniques"""
    print("Test dÃ©marrÃ©")
    import pdb; pdb.set_trace()  # Le test s'arrÃªte ici
    result = self.data_processor.add_indicators(self.test_df.copy())
    # ...
``` 